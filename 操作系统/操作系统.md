# 操作系统

[TOC]



## 1. 为什么需要操作系统

- 操作系统类似于一个协调者，它提供了一套接口和标准来抹平不同硬件的差异，这样只要操作系统一样，硬件能达到软件运行的最低标准，那么软件就可以运行，不需要考虑硬件的差异
- 同时，操作系统为所有程序分配资源，一切程序都要听从操作系统的调度，当它们需要访问资源时需要向操作系统申请权限由操作系统进行资源的分配，这样可以最大程序利用计算机资源的同时让各个程序互不冲突
- 操作系统屏蔽了底层硬件调度的细节，为应用程序提供基础，担当计算机用户和计算机硬件的中介，减少开发人员的开发难度，让非技术人员也能使用计算机系统
- 操作系统提供了便利，高效的使用体验



## 2. System/Application Programs

操作系统包含系统或应用程序

- 系统程序：

系统程序是用于直接命令或修改计算机硬件的软件

- 应用程序：

应用程序是用于执行特定任务，可供用户直接使用的程序或软件（比如office， 编译器，网页浏览器等）



## 3. 引导程序（Bootstrap Program）

电脑启动或重新运行时执行的第一个程序。引导程序存储于ROM(Read Only Memory)中，它必须知道如何加载操作系统并开始执行系统，它必须定位操作系统内核并将其加载到内存中



## 4. 中断（Interrupt）



**重要：没有中断技术就无法实现多段进程并发执行，中断可以让操作系统内核强行夺回CPU控制权**



![image-20240328192247110](Images\image-20240328192247110.png)

- **来源**：中断通常来源于硬件设备，如I/O操作完成、定时器溢出等。也可以由软件生成，称为软中断或软件中断。
- **异步性**：中断是异步发生的，与CPU的当前执行流程无关。
- **目的**：中断的主要目的是响应外部事件或内部事件，允许操作系统对这些事件进行处理，如处理硬件请求、执行定时任务等。
- **处理**：中断会导致当前执行流程被打断，操作系统保存当前的上下文（程序计数器、寄存器等），转而执行与中断对应的中断服务例程（ISR）。ISR执行完毕后，系统恢复被中断的任务。



一个事件的发生通常由从硬件或软件的中断发出信号，在CPU做某项工作时，有时硬件或软件可能会中断CPU正在执行的工作，并让它执行另一个工作，执行完成后回到当前工作继续执行，硬件通常使用系统总线向CPU发送信号来触发中断



中断处理通常涉及以下步骤：

1. **中断触发**：硬件设备向处理器发送中断信号。
2. **中断响应**：当前执行被暂停，处理器保存当前的上下文（如程序计数器、寄存器等）。
3. **执行中断服务例程（ISR）**：根据中断向量表找到对应的中断服务例程并执行。中断向量表是一个函数指针数组，每个元素指向一个ISR。
4. **中断返回**：ISR执行完成后，处理器恢复之前保存的上下文，并继续执行被中断的任务。

![image-20240328203642416](Images\image-20240328203642416.png)

### 4.1 内中断（也叫做异常）

与当前执行的指令**有关**，中断信号来自于CPU**内部**

![image-20240328193207708](Images\image-20240328193207708.png)

![image-20240328193307053](Images\image-20240328193307053.png)

分为三种：

- **陷阱、陷入（trap）**

​	由陷入指令（比如断点）引发，是应用程序故意引发的，要注意陷入指令**不是特权指令**

- **故障（fault）**

​	由错误条件引起的，可能被内核程序修复。内核程序修复故障后会把CPU使用权交给应用程序让它继续执行下去，比如：**缺页故障**

- **终止（abort）**

​	由致命错误引起，内核程序无法修复该错误，因此一般不再将使用权还给引发终止的应用程序，而是直接终止该应用程序。如：整数除0，非法使用特权指令

### 4.2 外中断

与当前执行的指令**无关**，中断信号来自于CPU**外部**

![image-20240328194109230](Images\image-20240328194109230.png)

### 4.3 系统调用

软件可以通过执行称为系统调用的特殊操作来触发中断

![image-20240328204041660](Images\image-20240328204041660.png)

![image-20240328204125690](Images\image-20240328204125690.png)

![image-20240328204324480](Images\image-20240328204324480.png)

系统调用协调分配资源防止出现意想不到的情况

### 4.4 CPU响应

当CPU被中断时会停止执行当前任务并立即将执行转移到固定位置（通常是中断服务程序的起始地址），每一个中断都有一个中断服务程序，当CPU执行完中断服务程序后会回到之前执行的任务继续执行



## 5. 进程

被加载到内存中执行的程序被称为“进程”

![image-20240328205819745](Images\image-20240328205819745.png)

### 5.1 进程状态

- #### 新建（New）

​	这是进程刚被创建时的状态。在这个状态下，操作系统为进程分配了必要的资源，例如内存空间、进程控制块（PCB）等。一旦分配完成，进程就可以转移到就绪状态。

- #### 就绪（Ready）

​	进程已准备好运行，并等待CPU时间。在这个状态下，进程已经拥有除CPU之外的所有必要资源，它被放在就绪队列中，等待操作系统的调度程序（Scheduler）将其选中并分配CPU时间片，以便执行。

- #### 运行（Running）

​	进程正在CPU上执行。一个进程在任意时刻只能在一个CPU上运行。当进程获得CPU时间后，它可以执行指令、进行计算等。运行状态的进程可能由于时间片用完、等待I/O操作或其他资源而转换到其他状态。

- #### 等待（Waiting 或 Blocked）

​	进程因为等待某种条件（如I/O操作完成、获取某种锁资源等）而暂停执行。在这个状态下，即使CPU空闲，进程也无法执行，直到其等待的条件得到满足。满足条件后，进程通常会转移到就绪状态，等待重新获得CPU时间。

- #### 终止（Terminated 或 Exit）

​	进程完成执行或者被操作系统终止。在这个状态下，进程的所有资源，包括打开的文件、占用的内存等，都会被操作系统回收。进程的PCB会保留一些终止信息直到操作系统处理完毕，之后也会被删除。

****

- #### 进程状态转换

进程在其生命周期中会经历多种状态，状态之间的转换通常由操作系统事件（如时间片用尽、I/O请求完成等）触发。这些状态转换反映了进程对系统资源的需求以及操作系统对资源的调度策略。

- #### 可选状态

除了上述基本状态外，一些系统可能还支持其他进程状态，例如：

**挂起就绪（Suspended Ready）**：进程已经准备好运行，但被移到外部存储，这通常发生在系统需要为其他进程释放内存时。

**挂起等待（Suspended Blocked）**：进程因等待某些条件而被挂起，通常是因为系统资源紧张。



### 5.2 PCB(Process Control Block)

每个进程在操作系统中都由一个进程控制块表示，简称PCB

- ### **`PCB主要包含以下信息：`**

1. **进程标识符（PID）**：每个进程的唯一标识。系统可以通过PID来区分不同的进程。
2. **用户ID（UID）**：进程所属用户ID
3. **进程状态**：进程当前的状态，如就绪（Ready）、运行（Running）、等待（Waiting）或终止（Terminated）等。
4. **程序计数器（PC）**：指向进程将要执行的**下一条指令的地址**。程序计数器使得操作系统能够在进程被中断后恢复执行。
5. **CPU寄存器信息**：保存进程执行状态的寄存器集，包括累加器、索引寄存器、栈指针等。当进程被中断时，其寄存器状态会被保存到PCB中，以便进程恢复执行时能够恢复这些值。
6. **CPU调度信息**：包括进程优先级、调度队列指针、调度参数等，这些信息用于进程调度决策。
7. **内存管理信息**：包括进程的地址空间信息，如页表、段表等，用于虚拟内存管理和物理地址映射。
8. **账户信息**：记录进程使用的CPU时间、实际使用时间等，用于统计、限制和计费等。
9. **I/O状态信息**：包括分配给进程的I/O设备列表、打开文件的列表等。这些信息用于管理进程的输入输出操作。

- ### `PCB的作用`

- **进程管理**：PCB是操作系统管理进程生命周期的基础。通过PCB，操作系统能够创建、调度、中断和终止进程。
- **进程切换**：在进行进程切换时，操作系统会保存当前进程的状态到其PCB中，并从另一个进程的PCB中恢复其状态，以实现多任务。
- **资源管理**：PCB帮助操作系统跟踪每个进程使用的资源，如内存、文件和I/O设备，确保资源正确分配和回收。
- **同步与通信**：PCB中的信息可以用于进程同步和通信机制，如信号量、消息队列等。



### 5.3 进程调度算法

- #### 先来先服务（FCFS, First-Come, First-Served）

**非抢占式**

这是最简单的调度算法。进程按照它们到达就绪队列的顺序被调度。一旦一个进程开始执行，它会持续运行，直到完成。FCFS的主要问题是平均等待时间可能很长，特别是当长进程运行在短进程之前时。

- #### 短作业优先（SJF, Shortest Job First）

**非抢占式**

在这个算法中，具有最短执行时间的进程首先获得CPU。这种非抢占式调度可以最小化平均等待时间。然而，它的主要问题是饥饿（长作业可能永远得不到调度），以及需要提前知道进程的执行时间。

- #### 最短剩余时间优先（SRTF, Shortest Remaining Time First）

**抢占式**

这是SJF的抢占式版本。如果一个新进程到达就绪队列，其预计执行时间小于当前运行进程的剩余时间，调度器会中断当前进程并将CPU分配给新进程。这种方法减少了等待时间，但也增加了上下文切换的次数。

- #### 时间片轮转（RR, Round Robin）

**抢占式**

RR调度算法将CPU时间分成固定长度的片段，称为时间片，然后将它们分配给就绪队列中的每个进程。每个进程运行一个时间片的长度。如果进程在时间片结束时尚未完成，它会被放回就绪队列的末尾。这种方法提高了响应性，但较短的时间片会导致较高的上下文切换开销。

- #### 优先级调度

在优先级调度算法中，每个进程都有一个优先级，调度器根据这些优先级分配CPU时间。具有最高优先级的进程首先获得CPU。**优先级调度可以是非抢占式的或抢占式的**（即一个更高优先级的进程到来时，可以中断当前进程）。这种方法的问题在于较低优先级的进程可能遭受**饥饿**。

- #### 多级反馈队列（MFQ, Multilevel Feedback Queue）

多级反馈队列是一种复杂的调度算法，它设有多个就绪队列，每个队列具有不同的优先级，同样**可以是非抢占式的或抢占式的**。进程根据其属性（如CPU使用时间、进程优先级等）被分配到不同的队列中。调度器首先考虑最高优先级的队列。MFQ旨在提供良好的响应时间，同时考虑到CPU利用率和吞吐量，但它的实现相对复杂。会导致**饥饿**

![image-20240328215840933](Images\image-20240328215840933.png)

![image-20240328220208646](Images\image-20240328220208646.png)



- #### **多级队列调度算法**

![image-20240328220413866](Images\image-20240328220413866.png)

****



![image-20240328214630004](Images\image-20240328214630004.png)



### 5.4 上下文切换

进程的上下文切换是指操作系统在切换从一个进程到另一个进程执行时所进行的一系列操作。这个过程涉及保存当前进程的状态（上下文）并恢复另一个进程的状态，以便新的进程可以从它上次停止的地方继续执行。

- #### `上下文的内容`

进程的上下文主要包括：

- **CPU寄存器状态**：包括通用寄存器、程序计数器（PC）、栈指针（SP）和状态寄存器等。
- **程序计数器**：指向进程将要执行的下一条指令的内存地址。
- **进程状态信息**：如进程的优先级、调度状态和进程ID等。
- **内存管理信息**：如页表、内存分配的状态等。
- **开放文件和I/O状态信息**：如文件描述符、I/O缓冲区等。

- #### `上下文切换的触发`

上下文切换可能由以下几种原因触发：

- **时间片用尽**：在基于时间片的调度算法中，当当前进程的时间片用尽时，操作系统会触发上下文切换。
- **I/O请求**：当进程进行I/O操作而被阻塞时，操作系统会切换到另一个进程。
- **高优先级进程就绪**：当一个高优先级的进程变为就绪状态时，操作系统可能会抢占当前运行的进程，进行上下文切换。
- **等待系统资源**：如信号量、互斥锁等。

- #### `上下文切换的过程`

1. **中断或系统调用**：上下文切换首先由系统调用、中断或异常触发。
2. **保存当前进程上下文**：操作系统保存当前进程的CPU寄存器和其他关键状态到其进程控制块（PCB）。
3. **选择新的进程**：调度器选择另一个进程执行。
4. **恢复新进程上下文**：操作系统从新选定的进程的PCB中恢复CPU寄存器和其他关键状态。
5. **执行新进程**：新的进程开始执行。

- #### `上下文切换的代价`

上下文切换是有代价的，包括：

- **时间开销**：保存和恢复进程状态需要时间，**这期间CPU不做任何有用的工作**。
- **缓存冷却**：新的进程可能会有不同的内存访问模式，导致CPU缓存的数据被替换，从而降低缓存的命中率。

- #### `优化上下文切换`

- **减少不必要的上下文切换**：通过优化调度策略，减少不必要的上下文切换。
- **使用线程**：线程间的上下文切换代价通常低于进程间的切换，因为线程共享同一地址空间和资源。



### 5.5 子进程

- #### `子进程的创建`

子进程通常通过特定的系统调用创建，如UNIX和Linux中的`fork()`，Windows中的`CreateProcess()`等。这些调用创建一个新的进程，该进程几乎是父进程的副本：

- **UNIX/Linux的`fork()`**：`fork()`调用创建一个新进程，该进程是调用进程的副本。新进程（子进程）从`fork()`调用之后的点开始执行，拥有与父进程几乎相同的内存映像，但有其自己的地址空间。
- **执行新程序**：子进程通常会使用如`exec()`系列函数（在UNIX/Linux中）来执行一个新的程序。`exec()`函数替换当前进程的内存空间，包括代码和数据，用新程序的代码和数据。

****

- #### `子进程的特性`

- **独立执行**：子进程拥有独立的地址空间，父进程和子进程的运行互不干扰。子进程的任何数据修改都不会影响父进程，反之亦然。
- **资源共享**：虽然子进程拥有独立的地址空间，但在某些操作系统中，子进程在创建时会继承父进程的资源，如打开的文件描述符（UNIX/Linux）。
- **通信**：子进程可以通过管道、信号、共享内存、消息队列等机制与父进程或其他进程通信。
- **同步**：父进程可以通过特定的系统调用（如`wait()`）等待子进程结束，这在父进程需要子进程完成某些任务后再继续执行时非常有用。

****

- #### `子进程的用途`

- **并行处理**：子进程可以用来并行执行多个任务，提高应用程序的性能。
- **简化编程模型**：通过将复杂任务分解为多个简单的子任务，并为每个子任务创建一个子进程，可以简化编程模型。
- **隔离**：子进程提供了一种隔离机制，使得不同任务在不同的进程空间中运行，增加了系统的稳定性和安全性。

****

- #### `注意事项`

- **资源管理**：大量使用子进程可能会导致资源（如内存和处理器）的过度使用，因此需要谨慎管理。
- **僵尸进程**：如果父进程没有通过`wait()`（或相关函数）等待子进程结束，子进程在结束时可能会成为僵尸进程，占用系统资源。
- **孤儿进程**：如果父进程在子进程之前结束，子进程将成为孤儿进程。在大多数系统中，孤儿进程会被init进程（或类似的进程）接管。

子进程是操作系统进程管理和并发编程中的一个重要概念，通过合理使用子进程，可以设计出高效、模块化和易于管理的应用程序。



### 5.6 僵尸进程

僵尸进程是已经完成执行但仍然在操作系统进程表中占据一个条目的进程。这种情况发生在子进程已经结束，但其父进程尚未通过调用`wait()`或`waitpid()`系统调用来回收子进程的状态信息时。僵尸进程不执行任何代码，也不消耗除了进程表条目之外的任何资源，但它们会占用有限的系统资源——进程号（PID），因此系统上不能有无限多的僵尸进程。

****

- ### 僵尸进程的产生

****

当子进程结束执行时，它会释放占用的大部分资源（如内存和打开的文件），但在操作系统的进程表中仍然保留一个条目，该条目包含了**进程的退出状态及其他一些终止信息**。操作系统保留这些信息是为了让父进程在稍后能够查询子进程的终止状态。如果父进程没有调用`wait()`或`waitpid()`来查询这些信息，子进程的进程表条目就不会被释放，从而导致僵尸进程的出现。



****

- ### 僵尸进程的问题

****

虽然单个僵尸进程消耗的资源非常少，但它们会占用进程号。在UNIX和Linux系统中，进程号是有限的，如果大量僵尸进程累积，可能会耗尽可用的进程号，导致系统无法启动新的进程。



****

- ### 如何处理僵尸进程

****

1. **父进程调用`wait()`或`waitpid()`**：这是预防和处理僵尸进程的最直接方法。这些系统调用使父进程暂停执行，直到它的一个或多个子进程结束，然后回收子进程的状态信息，释放进程表条目。
2. **使用信号**：父进程可以为`SIGCHLD`信号安装一个处理函数，该信号在子进程结束时由操作系统发送给父进程。在信号处理函数中，父进程可以调用`wait()`来回收子进程的状态信息。
3. **父进程结束**：如果僵尸进程的父进程结束，所有剩余的僵尸子进程将被init进程（**PID为1的进程**）领养，**init进程会定期调用`wait()`来回收任何僵尸进程的状态信息**。
4. **避免创建子进程**：在某些情况下，可以通过使用其他并发编程模型（如线程）来避免创建子进程，从而避免僵尸进程的问题。



### 5.7 孤儿进程

孤儿进程是指父进程结束或终止之后仍在运行的子进程。在父进程终止后，没有被终止的子进程将被init进程（在UNIX和Linux系统中，其进程ID通常为1）接管。init进程会定期执行`wait()`系统调用来回收孤儿进程的状态信息，从而防止孤儿进程成为僵尸进程。

****

- ### 孤儿进程的产生

****

- 当一个进程终止时，它的所有子进程将由init进程领养，这些子进程因此成为孤儿进程。这通常发生在父进程因为某种原因提前结束，而子进程还在后台运行时。
- 如果一个子进程是由一个已经终止的父进程创建的，但这个子进程还没有结束，那么这个子进程就变成了孤儿进程。

****

- ### 孤儿进程的处理

****

- 在UNIX和Linux系统中，init进程负责领养孤儿进程。init进程会定期执行`wait()`系统调用来等待孤儿进程结束，并回收它们的资源和状态信息。这样做是为了确保没有进程能永远处于未回收的状态，防止资源泄露。
- 孤儿进程通常不会对系统性能产生负面影响，因为它们仍然可以像正常进程一样运行。一旦它们结束执行，init进程将回收它们所占用的资源。

****

- ### 孤儿进程与僵尸进程的区别

****

- 孤儿进程和僵尸进程都是在特定条件下出现的进程状态，但它们之间有本质的区别。孤儿进程是已经失去父进程的子进程，但它仍然在运行并消耗资源；而僵尸进程是已经结束但其状态信息还未被父进程回收的进程，它不执行任何操作，不消耗除了进程表项之外的任何资源。
- 孤儿进程会被init进程领养，并在执行完成后由init进程清理；僵尸进程则需要其原父进程（或在某些系统中可能是init进程）通过调用`wait()`或`waitpid()`来回收其资源。



### 5.8 进程间通信（IPC）

进程间通信（Inter-Process Communication, IPC）是指在不同进程之间传递数据或信号的机制。由于每个进程在现代操作系统中一般都运行在独立的地址空间中，它们无法直接访问对方的变量或数据结构。因此，操作系统提供了多种IPC机制来实现进程间的数据交换和同步。

****

- ### **为什么需要进程间通信**

****

- ####  **资源共享**

在多个进程需要访问相同的资源（如文件、数据库或内存中的数据结构）时，IPC 提供了一种协调机制，确保资源访问的正确性和一致性。没有适当的同步和通信机制，可能会导致数据损坏或不一致。

- #### 计算分工

在一些复杂的应用中，不同的任务可能被分配给不同的进程，这些进程可能运行在同一台机器上或分布在网络中的不同机器上。IPC 允许这些进程彼此通信，协调工作流程，使得任务能够高效地分工合作完成。

- #### 模块化和解耦

通过IPC，系统可以被设计成多个独立的模块或服务，每个模块或服务运行在其自己的进程中。这种模块化设计使得系统更易于理解、开发和维护，同时也提高了系统的可扩展性和可靠性。

- #### 并发性

IPC 支持系统并发执行多个任务，提高资源利用率和系统吞吐量。通过在多个进程间分配工作，可以利用多核处理器的并行计算能力，从而提升性能。

- #### 安全和隔离

不同进程在操作系统层面上是隔离的，它们拥有独立的地址空间。这种隔离提供了一定程度的安全保护，因为一个进程无法直接访问另一个进程的内存空间。IPC 提供了一种安全的方式，允许这些隔离的进程按需共享数据。

- #### 灵活性和可扩展性

IPC 机制使得在不同机器上运行的进程之间的通信成为可能，这对于构建分布式系统和微服务架构非常重要。这些系统能够通过网络进行通信，从而实现更高的灵活性和可扩展性。



### 5.9 进程间通信的方法

#### 5.9.1 管道（Pipes）

管道主要有两种类型：

1. **匿名管道（Anonymous Pipes）**：
   - **仅能用于有共同祖先的进程间通信**（如父子进程），因为它们没有与之关联的文件系统名字。
   - 一般用于单向通信——数据只能从一端流向另一端。
2. **命名管道（Named Pipes 或 FIFO）**：
   - 可以在没有共同祖先的任意进程之间进行通信，因为它们在文件系统中有一个名字。
   - 支持单向或双向通信。
   - 由于它们在文件系统中有一个明确的名字，所以不同的进程可以通过打开同一个命名管道来进行通信。

- #### `工作原理`

匿名管道的典型使用流程如下：

1. **创建管道**：进程调用`pipe()`系统调用创建一个管道，该调用返回两个文件描述符：一个用于读取（管道的读端），另一个用于写入（管道的写端）。
2. **使用管道**：创建管道后，原始进程通常会通过`fork()`系统调用创建一个子进程。父子进程中的一个将关闭读端描述符，另一个关闭写端描述符，这样就形成了一个单向通道。
3. **数据传输**：写端进程可以通过写端描述符向管道中写入数据，而读端进程则可以从读端描述符读取数据。**数据是按照写入的顺序被读取的。（先进先出）**
4. **关闭管道**：当通信完成后，进程会关闭相应的文件描述符。当管道的所有写端都被关闭后，读取操作会返回文件结束符（EOF），以示数据传输完成。

命名管道的使用流程与匿名管道类似，不同之处在于它首先需要通过`mkfifo()`系统调用（或类似的命令）在文件系统中创建一个管道文件，然后进程通过打开这个文件进行读写操作。

- #### `特点`

  - **简单性**：管道是一种简单的通信机制，容易实现和使用。

  - **单向数据流**：标准管道只支持单向数据流。如果需要双向通信，需要创建两个管道。

  - **数据的顺序性**：管道中的数据按照写入的顺序被读取。

  - **阻塞和非阻塞**：管道的读取和写入操作可以是阻塞的，也可以是非阻塞的，这取决于管道的配置和当前状态。

- #### `限制`

  - **缓冲区大小**：管道有一个**固定的**缓冲区大小（通常为几KB），当缓冲区满时，写操作会阻塞；当缓冲区空时，读操作会阻塞。

  - **生命周期和范围**：匿名管道的生命周期和可见范围通常限于创建它们的进程及其子进程。

  - **数据结构**：管道不适合传递复杂的数据结构，因为它们仅仅是字节流，没有内置的消息边界或结构化机制。



#### 5.9.2 消息传递（Message Passing）（消息队列）

****

- #### 基本概念

****

进程间的消息传递（Message Passing）是一种在计算机科学中广泛使用的通信机制，尤其是在并发编程和分布式系统中。这种机制允许进程之间**通过发送和接收消息来交换数据**，而不是直接共享内存。这样做有助于减少进程间的依赖性，增强系统的模块化，并有助于提高程序的可维护性和扩展性。

- #### `基本概念`

- **消息**：消息是在进程间传递的数据单元。消息可以是简单的如信号、数字，也可以是复杂的如数据结构或对象，信息的大小可以是固定的也可以是可变的
  - **固定大小：**系统级实现简单，但是编程非常困难，要考虑信息大小超过固定大小的情况
  - **可变大小：**系统级实现复杂，但是编程相对简单

- **发送和接收**操作

  ：进程使用发送操作将消息传递给另一个进程，并使用接收操作来接收其他进程发送的消息。发送和接收可以是同步的也可以是异步的。

  - **同步发送**（Synchronous Send）：发送进程在消息被接收之前被阻塞。
  - **异步发送**（Asynchronous Send）：发送进程在发送消息后可以继续执行，而不需要等待消息被接收。
  - **同步接收**（Synchronous Receive）：接收进程在接收到消息之前被阻塞。
  - **异步接收**（Asynchronous Receive）：接收进程可以继续执行，并在消息到达时被通知。

- #### `通信模型`

1. **直接通信**：在直接通信模型中，每个发送或接收操作都需要明确指定对方进程的标识符。这意味着发送进程需要知道接收进程的标识，反之亦然。

   - send (P, message)        //发送信息给进程P
   - receive (Q, message)   //从进程Q接收到信息

   链接是在每一对想要通信的进程之间自动创建的，进程只需要知道对方的标识符就能通信；链接只与特定的两个进程相关，对于每一对通信进程，一定存在一个链接；

2. **间接通信**：在间接通信模型中，消息是通过共享的数据结构传递的，进程通过这些结构进行通信而不是直接引用对方，类似于每个进程都有一个“**邮箱**”可以查看信息。这种方式更加灵活，允许多个生产者和消费者之间的通信。

   - send (A, message)        //发送信息到邮箱A

   - receive (A, message)   //从邮箱A接收到信息

当两个进程都有同一个邮箱时建立链接，一个链接可以与多个进程相关，只要它们都有同样的邮箱就行。 进程可以有多个不同的邮箱所以可以有多个不同的链接，每个链接与邮箱绑定

**Example：**如果进程P1，P2，P3共享一个邮箱，当P1向邮箱发送一个信息，可以设计该信息只能被特定进程接收



- #### `工作原理`

  1. **创建消息队列**：一个进程会创建一个消息队列，并为它指定一个唯一的标识符或名称，其他进程可以使用这个标识符来访问队列。

  2. **发送消息**：进程可以将消息放入队列中。每个消息都可以被赋予一个特定的类型或优先级，以便接收方可以根据这些属性选择性地接收消息。

  3. **接收消息**：进程可以从队列中读取消息。接收操作可以是阻塞的，也可以是非阻塞的，并且接收方可以选择接收特定类型的消息。

  4. **管理队列**：系统或进程可以对消息队列进行查询、修改配置或删除队列等管理操作。

  5. **随机查询：**消息队列可以实现消息的**随机查询**，不一定非要以先进先出的次序读取消息，也可以按消息的类型读取。比有名管道的先进先出原则更有优势。

- #### `特点`

  - **解耦**：生产者和消费者之间不需要直接通信，它们只需要关注消息队列，这有助于降低系统组件之间的耦合度。

  - **异步通信**：消息队列允许进程继续执行而不需要等待消息的即时响应，有助于提高应用的吞吐量和响应性。

  - **持久性**：许多消息队列系统支持将消息持久化到磁盘，这意味着即使系统崩溃，消息也不会丢失。

  - **灵活性**：消息队列可以根据优先级、类型或其他属性来选择性地处理消息，提供了高度的灵活性。
  
- #### `缺点`

  - **上下文切换：**当进程读写消息队列里的消息时，需要从用户态切换到内核态，**如果数据量较大，使用消息队列就会造成频繁的系统调用，也就是需要消耗更多的时间以便内核介入**。




#### 5.9.3 共享内存（共享存储）

共享内存是一种高效的进程间通信（IPC）机制，它允许两个或多个进程共享一个给定的内存区域。由于它直接在进程间共享数据，而不是通过操作系统交换数据，因此共享内存通常被认为是最快的IPC方法之一。

该机制可使不同的进程共享主存中的某一个区域，且使该区域出现（映射）在多个进程的虚地址空间中。另一方面，一个进程的虚地址空间中又可连接多个共享存储区，每个共享存储区都有自己的名字。当进程间欲利用共享存储区进行通信时，必须先在主存中建立一共享存储区，然后将它**附接到自己的虚地址空间**上。此后，进程对该区的访问操作，与对其虚地址空间的其它部分的操作完全相同。为避免出错，各个进程对共享内存的访问必须是互斥的

- #### `工作原理`

1. **初始化：** 一个进程创建一个共享内存段，这个内存段对该进程是可见的，就像进程的私有内存一样。
2. **附加：** 其他进程可以将同一个共享内存段附加到自己的地址空间，使得这段内存变得对它们也是可访问的。
3. **使用：** 一旦共享内存被附加，进程就可以像访问自己的正常内存一样访问共享内存。进程可以读写存储在共享内存中的数据，就如同操作自己的局部变量一样。
4. **分离与销毁：** 进程使用完共享内存后，可以将其从自己的地址空间中分离。当所有使用它的进程都已分离，且不再需要该内存段时，可以将其销毁。

- #### `优点`

- **性能：** 由于不需要进程间发送消息或**进行上下文切换**（注意：共享内存只有建立共享内存区域时需要系统调用），共享内存是最快的IPC方法。

- **灵活性：** 共享内存提供了一种灵活的方式，使得多个进程可以以任何所需的格式访问和修改数据。

- #### `缺点`

- **同步问题：** 使用共享内存时，必须仔细处理同步问题，以避免竞态条件和数据不一致。这通常需要使用信号量、互斥锁或其他同步机制。

- **复杂性：** 管理共享内存（分配、同步访问、清理）比使用其他IPC机制更加复杂。

- **安全与隔离：** 共享内存**破坏了进程间的内存隔离**，可能导致安全问题。



#### 5.9.4 信号量 (Semaphores）

​	**信号量是一种低级通信方式**，因为它只能用于进程间协调不能用于进程间传输资源，主要用于进程间的同步和互斥，特别是控制对共享资源的访问。信号量可以是二元的（也称为互斥锁）或可以有更高的值。详细可查看5.11



#### 5.9.5 信号 (Signals)

一种比较简单的通信方式，用于告知接收进程某个事件已经发生。信号是一种软件中断，用于**处理异步事件**。

***基本概念***

- **信号的来源**：信号可以由用户通过终端发送（例如，Ctrl+C通常会发送`SIGINT`信号以中断当前进程），由操作系统内核发送（例如，当进程试图除以零时发送`SIGFPE`），或者由一个进程发送给另一个进程（使用`kill`命令或`kill()`系统调用）。
- **信号的类型**：存在多种标准信号，每种信号代表不同的事件类型。例如，`SIGINT`代表中断信号，`SIGKILL`用于强制终止进程，`SIGALRM`可以由定时器产生，`SIGSEGV`指示内存访问违规等。

***处理信号***

当一个信号被发送到一个进程时，该进程可以采取三种基本方式之一来处理它：

1. **默认处理**：每种信号都有一个默认的处理行为，这可能是终止进程、忽略信号、暂停或继续进程执行，或产生核心转储文件等。如果进程没有明确指定如何处理某个信号，就会采取默认的处理方式。
2. **忽略信号**：进程可以选择忽略某些信号（除了`SIGKILL`和`SIGSTOP`，这两个信号不能被忽略或捕获，以确保系统的稳定性和响应性）。
3. **捕获信号**：进程可以注册一个信号处理函数，当特定信号到达时，操作系统将调用该函数。这允许进程异步地响应外部事件，例如清理资源、保存状态、或进行特定的信号相关操作。

***发送信号***

信号可以通过多种方式发送：

- **用户通过终端**：例如，使用`Ctrl+C`通常发送`SIGINT`信号，`Ctrl+Z`发送`SIGTSTP`信号。
- **使用`kill`命令或`kill()`系统调用**：可以从另一个进程发送信号。
- **由操作系统自动发送**：当发生硬件异常、定时器到期等事件时。



#### 5.9.6 套接字（Socket）

Socket通过IP地址和端口号来标识

服务器通过监听来等待传入的客户端请求，一旦收到请求，服务器就会接受来自客户端Socket的连接

从计算机网络层面来说，**Socket 套接字是网络通信的基石**，是支持 TCP/IP 协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：**连接使用的协议，本地主机的 IP 地址，本地进程的协议端口，远地主机的 IP 地址，远地进程的协议端口。**





### 5.10 进程同步和互斥

![image-20240328220839973](Images\image-20240328220839973.png)

**同步：**

​	同步亦称直接制约关系，它是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上**协调**它们的**工作次序**而产生的制约关系。进程间的直接制约关系就是源于它们之间的相互合作。

****

![image-20240328221319043](Images\image-20240328221319043.png)

![image-20240328221506574](Images\image-20240328221506574.png)

![image-20240328223829592](Images\image-20240328223829592.png)

#### 5.10.1 进程互斥的软件实现方法

****

***单标志法***

****

**算法思想：**两个进程在**访问完临界区后**把使用临界区的权限转交给另一个进程。也就是说**每个进程进入临界区的权限只能被另一个进程赋予**

~~~c++
int turn = 0; //turn表示当前允许进入临界区的进程号 turn变量背后的逻辑：表达“谦让”
~~~

~~~c++
//P0进程
while(turn != 0);   //进入区
critical section;   //临界区
turn = 1;           //退出区
remainder section;  //剩余区

//P1进程
while(turn != 1);   //进入区
critical section;   //临界区
turn = 0;           //退出区
remainder section;  //剩余区
~~~

**问题：**只能按 P0→P1→P0 →P1→…这样轮流访问。这种必须“轮流访问”带来的问题是，如果此时允许进入临界区的进程是 P0，而 P0一直不访问临界区，那么虽然此时临界区空闲，但是并不允许P1访问。因此，**单标志法**存在的**主要问题**是：**违背“空闲让进”原则**



****

***双标志先检查法***

****

**算法思想：**设置一个布尔型数组flag，数组中各个元素用来**标记各进程想进入临界区的意愿**，比如“`flag[0]=ture`”意味着进程P0现在想要进入临界区。每个进程在进入临界区之前先检查当前有没有别的进程想进入临界区，如果没有，则把自身对应的标志flag[i]设为true，之后开始访问临界区

~~~c++
bool flag[2];            //表示进入临界区意愿的数组 背后的逻辑：“表达意愿”
flag[0] = false;
flag[1] = false;         //刚开始设置为两个进程都不想进入临界区
~~~

~~~c++
//P0进程
while(flag[1]);          //[1]如果此时P1想进入临界区，P0就一直循环等待
flag[0] = true;          //[2]标记P0进程想进入临界区
critical section;        //[3]访问临界区
flag[0] = false;         //[4]访问完临界区，修改标记为P0不想使用临界区
remainder section;

//P1进程
while(flag[0]);          //[5]如果此时P0想进入临界区，P1就一直循环等待
flag[1] = true;          //[6]标记P1进程想进入临界区
critical section;        //[7]访问临界区
flag[1] = false;         //[8]访问完临界区，修改标记为P1不想使用临界区
remainder section;
~~~

**问题：**并发执行可能会出问题。若按照 [1] [5] [2] [6] [3] [7]...的顺序执行，P0 和 P1将会同时访问临界区。因此，双标志先检查法的**主要问题**是:**违反“忙则等待”原则**。

**原因：进入区**的“检查”和“上锁”两个处理**不是一气呵成的**。“检查”后，“上锁”前可能发生进程切换。



****

***双标志后检查法***

****

**算法思想：**双标志先检查法的改版。前一个算法的问题是先“检查”后“上锁”，但是这两个操作又无法一气呵成，因此导致了两个进程同时进入临界区的问题。因此，人们又想到先“上锁”后“检查的方法，来避免上述问题。

~~~c++
bool flag[2];            //表示进入临界区意愿的数组 背后的逻辑：“表达意愿”
flag[0] = false;
flag[1] = false;         //刚开始设置为两个进程都不想进入临界区
~~~

~~~c++
//P0进程
flag[0] = true;          //[1]标记P0进程想进入临界区
while(flag[1]);          //[2]如果此时P1想进入临界区，P0就一直循环等待
critical section;        //[3]访问临界区
flag[0] = false;         //[4]访问完临界区，修改标记为P0不想使用临界区
remainder section;

//P1进程
flag[1] = true;          //[5]标记P1进程想进入临界区
while(flag[0]);          //[6]如果此时P0想进入临界区，P1就一直循环等待
critical section;        //[7]访问临界区
flag[1] = false;         //[8]访问完临界区，修改标记为P1不想使用临界区
remainder section;
~~~

**问题：**同样是并发执行会出现问题。若按照[1] [5] [2] [6]...的顺序执行，P0和P1将都无法进入临界区因此，双标志后检查法虽然**解决了“忙则等待”**的问题，但是**又违背了“空闲让进”和“有限等待“**原则，会因各进程都长期无法访问临界资源而**产生“饥饿”**现象。



****

***Peterson算法***

****

**算法思想：**结合双标志法、单标志法的思想。如果双方都争着想进入临界区，那可以让进程尝试“孔融让梨”(谦让)。做一个有礼貌的进程。

~~~c++
bool flag[2];            //表示进入临界区意愿的数组 背后的逻辑：“表达意愿”
int turn = 0;            //turn表示当前允许进入临界区的进程号 
~~~

~~~c++
//P0进程
flag[0] = true;          //[1] 表示自己想进入临界区
turn = 1;                //[2] 可以优先让对方进入临界区
while(flag[1]&& turn==1);//[3] 对方想进，且最后一次是自己“让梨”，那自己就循环等待
critical section;        //[4]
flag[0]= false;          //[5] 访问完临界区，表示自己已经不想访问临界区了
remainder section;

//P1进程
flag[1]= true;           //[6] 表示自己想进入临界区
turn = 0;                //[7] 可以优先让对方进入临界区
while(flag[0]&& turn==0);//[8] 对方想进，且最后一次是自己“让梨”，那自己就循环等待
critical section;        //[9]
flag[1]= false;          //[10]访问完临界区，表示自己已经不想访问临界区了
remainder section;
~~~

**”谁最后说了客气话，谁就失去了行动的优先权“**

**问题：**Peterson 算法用软件方法解决了进程互斥问题，**遵循了空闲让进、忙则等待、有限等待 三个原则**，但是依然**未遵循让权等待**的原则，出现“**忙等**”



#### 5.10.2 进程互斥的硬件实现方法

中断屏蔽方法，TS/TSL指令，Swap/XCHG指令



#### 5.10.3 自旋锁

像Peterson算法，中断屏蔽算法等需要忙等的算法用到的锁被称作自旋锁

- **特性：**需忙等，进程时间片用完才下处理机，**违反“让权等待”**
- **优点：**等待期间不用切换进程上下文，多处理器系统中，若上锁的时间短，则等待代价很低
- 常用于多处理器系统，一个核忙等，其他核照常工作，并快速释放临界区
- 不太适用于单处理机系统，根据算法的不同忙等的过程中可能一直等待导致系统崩溃
- 在多进程（线程）任务中，如果上下文切换的耗时大于忙等的耗时则应该采用自旋锁



### 5.11 信号量

为了解决**自旋锁无法实现“让权等待”**这一问题，Dijkstra提出了一种新的实现进程互斥、同步的方法——**信号量机制**

![image-20240328234947389](Images\image-20240328234947389.png)



#### 5.11.1 整型信号量

 **依旧会导致忙等**

![image-20240329000320537](Images\image-20240329000320537.png)

#### 5.11.2 记录型信号量

![image-20240329000637829](Images\image-20240329000637829.png)

想要访问计算机资源但是资源被占用的进程会被发送信号**从运行态变为阻塞态**，当其他占用该资源的进程释放该资源后会**查看记录**给因为等待该资源而进入阻塞态的进程发送信号**通知它们进入就绪态**



![image-20240329001416936](Images\image-20240329001416936.png)

![image-20240329001711890](Images\image-20240329001711890.png)



#### 5.11.3 信号量（互斥量）实现互斥（互斥锁）

**二元信号量也被称为互斥量**，可以用来创建互斥锁，**互斥锁相比自旋锁不会出现忙等待的问题**，必须对同一进程（线程）实现PV操作，必须成对出现，当某一进程无法获得资源时**会进入阻塞状态**

![image-20240329002348175](Images\image-20240329002348175.png)

#### 5.11.4 信号量实现同步

和互斥的区别就是**互斥操作是最开始的时候有资源**，**同步操作最开始的时候没资源**，要等先运行的程序执行完才会提供资源，**前V后P**

![image-20240329002900364](Images\image-20240329002900364.png)



#### 5.11.5 信号量实现前驱关系

**前驱关系问题本质就是多级同步问题**

![image-20240329003517081](Images\image-20240329003517081.png)

### 5.12 管程



### 5.13 死锁

#### 5.13.1 定义

**死锁：**在并发环境下，各进程（线程）因竞争资源而造成的一种**互相等待对方手里的资源，导致各进程都阻塞无法向前推进**的现象

![image-20240329014101606](Images\image-20240329014101606.png)

#### 5.13.2 进程死锁、饥饿、死循环的区别

**死锁：**各进程互相等待对方手里的资源，导致各进程都阻塞，无法向前推进的现象。

**饥饿：**由于长期得不到想要的资源，某进程无法向前推进的现象。比如:在短进程优先(SPF)算法中，若有源源不断的短进程到来，则长进程将一直得不到处理机，从而发生长进程“饥饿”

**死循环：**某进程执行过程中一直跳不出某个循环的现象。有时是因为程序逻辑 bug导致的，有时是程序员故意设计的。

![image-20240329014255044](Images\image-20240329014255044.png)

#### 5.13.3 产生的必要条件

![image-20240329022202796](Images\image-20240329022202796.png)

#### 5.13.4 为什么会发生死锁

![image-20240329022427970](Images\image-20240329022427970.png)



#### 5.13.5 处理策略-预防死锁

**简述：**破坏死锁的四个必要条件中的一个或几个

****

***破坏互斥条件***

****

![image-20240329023216289](Images\image-20240329023216289.png)

****

***破坏不可剥夺条件***

****

<img src="Images\image-20240329023505266.png" alt="image-20240329023505266"  />

****

***破坏请求和保持条件***

****

![image-20240329023726008](Images\image-20240329023726008.png)

****

***破坏循环等待条件***

****

![image-20240329024037452](Images\image-20240329024037452.png)



#### 5.13.6 处理策略-避免死锁（银行家算法）

**简述：**用某种方法防止系统进入不安全状态从而避免死锁（银行家算法）

****

**安全序列，不安全状态**

****

![image-20240329024419148](Images\image-20240329024419148.png)

**将30亿借给B会不安全，所以不能借**

![image-20240329024700817](Images\image-20240329024700817.png)

**将20亿借给A是安全的，可以借**



![image-20240329025035667](Images\image-20240329025035667.png)

****

***银行家算法***

****

**分配前判断会不会进入不安全状态**

![image-20240329025346658](Images\image-20240329025346658.png)

![image-20240329025535864](Images\image-20240329025535864.png)

**算法逻辑：**

![image-20240329030027085](Images\image-20240329030027085.png)

**安全性算法步骤：**

- 检查当前的剩余可用资源是否能满足某个进程的最大需求，如果可以，就把该进程加入安全序列，并把该进程持有的资源全部回收。
- 不断重复上述过程，看最终是否能让所有进程都加入安全序列。



#### 5.13.7 处理策略-检测和解除

**简述：**允许死锁发生，不过操作系统会负责检测出死锁的发生，然后采取某些措施解除死锁

需要两种算法：**死锁检测算法和死锁解除算法**



## 6. 线程

线程是进程内的执行单元，一个进程可以有多个线程，线程还是CPU调度的最小单元 

### 6.1 线程状态

线程也有**类似于进程的状态**，这些状态描述了线程在其生命周期中的不同阶段。线程作为轻量级的执行单元，共享进程的资源如代码段，内存和文件句柄，但它们拥有自己的执行上下文，包括**程序计数器、寄存器集和栈**。线程的状态管理对于多线程程序的性能和资源利用至关重要。

1. **新建（New）**：线程已经被创建，但尚未开始执行。在这个状态下，线程的资源和执行上下文已经准备好，等待被线程调度器选中分配CPU时间。
2. **就绪（Runnable/Ready）**：线程准备好执行，等待获取CPU时间。就绪状态的线程位于线程调度器的就绪队列中，一旦调度器选择了某个线程，它就会转移到运行状态。
3. **运行（Running）**：线程正在CPU上执行。线程从就绪状态转移到运行状态后，会执行其任务直到完成，或者因某些原因被迫暂停。
4. **阻塞（Blocked）/等待（Waiting）**：线程由于某些原因无法继续执行，进入等待状态，直到某个事件发生。这可能是因为I/O操作、获取同步锁失败或其他等待资源。阻塞状态的线程不会消耗CPU资源，它们在等待事件完成后会转移到就绪状态。
5. **超时等待（Timed Waiting）**：允许线程在指定的时间内等待某个条件的发生。如果条件在超时时间内得到满足，线程会继续执行；如果超时时间到达而条件仍未满足，线程可以选择继续等待、重试或执行其他操作。这种机制对于避免线程永久阻塞在某个条件上非常有用，特别是在条件可能因为外部因素而长时间不满足的情况下。
6. **终止（Terminated）**：线程的执行已经完成或者被强制终止。一旦线程终止，它所占用的所有资源将被释放，线程对象变为不可再用。



### 6.2 TCB(Thread Control Block)

TCB（Thread Control Block）是操作系统中用于存储线程信息的数据结构。它包含了管理和调度线程所需的所有关键信息。每个线程都有一个与之对应的TCB，操作系统通过TCB来控制线程的执行，包括线程的创建、暂停、恢复和终止等。

进程控制块（PCB）或与之类似的结构会**包含指向其内部每个线程TCB的指针**，以便操作系统可以管理和调度这些线程。

- ### TCB 包含的主要信息：

1. **线程标识符**：唯一标识每个线程的信息，通常包括线程ID等。
2. **线程状态**：表示线程当前的执行状态，如就绪（Ready）、运行（Running）、等待（Waiting）、终止（Terminated）等。
3. **程序计数器（PC）**：存储线程下一条要执行指令的地址。它指示了线程在其执行代码中的当前位置。
4. **寄存器集**：保存线程当前执行上下文的处理器寄存器的值，包括累加器、索引寄存器、栈指针和基指针等。
5. **堆栈指针**：指向线程私有栈的顶部。线程栈用于存储局部变量、返回地址和函数调用的历史。
6. **线程优先级**：用于调度决策，表示线程的执行优先级。在多线程调度中，通常优先级高的线程会被优先调度执行。
7. **线程特定的存储区**：用于存储线程局部存储（TLS）的数据，这些数据对其他线程不可见。
8. **信号掩码**：定义了线程对哪些信号的响应方式，即哪些信号被阻塞，哪些信号可被线程接收。
9. **上下文信息**：包括线程的寄存器集、程序计数器和堆栈指针，足够在线程被重新调度运行时恢复其执行状态。
10. **资源指针**：指向线程所使用的各种资源，如文件描述符、打开的文件句柄、分配的内存区域等。

### 6.3 线程共享内容

****

- #### **内存空间**

****

- **代码段**：所有线程共享同一进程的代码段，意味着它们**执行相同的程序代码**。
- **数据段**：包括全局变量和静态变量，这部分内存也被进程内的所有线程共享。
- **堆**：动态分配的内存（如C/C++中的`malloc`或`new`分配的内存，Java中的对象）位于堆区，这也是由进程的所有线程共享的。

****

- #### 操作系统资源

****

- **文件描述符**：打开的文件、管道、套接字等由进程管理，因此它们在同一进程的所有线程之间共享。
- **信号和信号处理程序**：信号是一种进程级别的通信机制，信号及其处理函数对同一进程下的所有线程都是可见和共享的。

****

- #### 进程属性

****

- **环境变量**：进程的环境变量设置对所有线程都是共享的。
- **当前目录**：进程的工作目录是共享的，对文件系统的相对路径操作会影响到同一进程内的所有线程。

****

- #### 系统资源

****

- **进程ID**：虽然每个线程有自己的线程ID，但它们共享相同的进程ID。
- **父进程ID**：与进程ID相同，所有线程都能访问相同的父进程ID。
- **进程的用户ID和组ID**：对操作系统资源的访问权限由进程的用户ID和组ID控制，这对进程内的所有线程都是一样的。



### 6.4 线程独占内容

- ###  **线程栈**

每个线程拥有自己的栈空间，用于存储函数调用时的局部变量、函数参数、返回地址和栈帧等。这个独立的栈空间是线程执行函数调用的基础，确保了线程函数调用的独立性和安全性。

- ### **线程上下文（寄存器状态）**

线程的上下文包括程序计数器（PC）、堆栈指针和其他处理器寄存器的状态。这些寄存器保存了线程当前执行状态的信息，包括当前执行指令的位置（PC）、局部变量和函数参数的地址（堆栈指针）等。当线程被操作系统挂起时，它的上下文会被保存，以便线程恢复执行时可以从中断点继续。

- ###  **线程局部存储（TLS）**

线程局部存储是一种允许数据在多个线程中有各自独立实例的机制。每个线程访问TLS变量时，实际上访问的是自己独有的数据副本。这对于保持线程的状态信息、避免全局变量的共享使用非常有用。

- ### **线程ID**

每个线程有一个唯一的线程ID，用于标识和管理线程。虽然线程ID本身可以被进程中的其他线程访问，但每个ID唯一对应一个线程，因此可以视为线程的“独占资源”。

- ### **线程的信号屏蔽**

在某些操作系统中，线程可以屏蔽（忽略）或处理特定的信号。这些信号屏蔽状态是线程特有的，不同线程可以有不同的信号屏蔽配置。

- ### 线程的优先级和调度策略

线程的优先级和调度策略决定了线程相对于进程中其他线程的执行优先级。这些属性可以被视为线程独占的，因为它们影响的是线程自身的调度和执行。



### 6.5 多线程的优点

- ###  **并发执行**

多线程允许程序同时执行多个任务。这可以显著提高应用程序的吞吐量和效率，尤其是在多核处理器上，不同的线程可以在不同的核上并行运行。

- ### **资源共享**

线程共享其父进程的内存空间和资源，这使得线程间的数据共享变得容易和高效。与进程相比，线程间的通信成本更低，因为它们可以直接访问相同的内存空间。

- ### **响应性**

在一些需要快速响应用户输入或其他事件的应用中，多线程可以在一个线程等待或执行长时间操作时，保持应用的响应性。例如，图形用户界面（GUI）应用程序通常在一个单独的线程中处理用户界面事件，以保持界面响应用户操作。

- ### **更好的系统资源利用**

多线程允许应用程序在等待I/O操作（如读取文件或网络通信）完成时继续执行其他任务。这种I/O并发可以显著提高应用程序对系统资源的利用率。

- ### **更简洁的异步编程模型**

在处理异步操作时，多线程提供了一种比事件驱动或回调更直观、更易于理解和管理的编程模型。通过线程，可以将异步操作表达为顺序代码，这简化了编程模型，减少了代码复杂性。

- ### **分离复杂任务**

复杂或耗时的任务可以分解为多个线程，每个线程处理任务的一部分。这不仅可以提高任务的执行速度（通过并行处理），还可以使代码组织更清晰，每个线程可以专注于执行一个具体的子任务。

- ### **优化多核处理器性能**

现代计算机系统通常配备有多核处理器。多线程使得应用程序能够通过在多个核心上并行执行线程来充分利用这些处理器资源，从而提高整体性能。



### 6.6 多线程和多子进程的对比

开启多个子进程可以在某种程度上达到类似多线程的效果，特别是在**并行处理和任务分解**方面。然而，进程与线程之间存在本质区别，这些区别导致它们在应用程序设计和性能方面具有不同的特点和考虑事项。

- ### 相似之处

1. **并行性**：无论是多线程还是多进程，都可以利用多核CPU的能力，通过并行执行来提高程序的执行效率。
2. **任务分解**：复杂或耗时的任务可以在多个线程或进程之间分解，实现任务的并发执行。

- ### 不同之处

1. **资源共享与隔离**：
   - **线程**：线程共享其父进程的内存空间和资源，如代码段、数据段和打开的文件等。这使得**线程间的通信和数据共享更为高效**，但也需要额外的同步机制来避免竞态条件。
   - **进程**：每个进程拥有独立的内存空间和资源。这种隔离提高了进程间的安全性和稳定性，但使得进程间的通信更为复杂，通常需要使用进程间通信（IPC）机制，如管道、消息队列、共享内存等。
2. **开销**：
   - **线程**：**线程的创建、切换和销毁的开销通常小于进程**，因为线程共享大部分进程资源。
   - **进程**：每个进程都有独立的地址空间，创建和切换进程的开销通常大于线程。此外，进程间的通信开销也大于线程间的通信。
3. **容错性与隔离**：
   - **线程**：线程间共享相同的地址空间，一个线程的崩溃可能影响到同一进程内的其他线程。
   - **进程**：进程间彼此隔离，一个进程的崩溃不会直接影响到其他进程。

- ### 使用场景

- **多线程**：适用于高度并发、需要大量数据共享和通信的应用场景，如服务器应用、复杂的计算任务等。
- **多进程**：适用于需要高度隔离、稳定性更重要的场景，或者是需要运行不同程序的情况，**如浏览器的每个标签页运行在独立进程中**，以提高整体应用的稳定性和安全性。



### 6.7 多线程模型

#### 6.7.1 用户级线程

![image-20240328185823875](Images\image-20240328185823875.png)

操作系统**看不到用户级线程**，用户级线程可以理解为用户用**线程库**在逻辑上模拟了线程的功能，也因此当用户级线程阻塞时操作系统无能为力，也因为用户级线程是模拟了线程的功能，所以用户级线程的管理由应用程序的线程库来负责，**不需要切换到内核态**



- **优点：**

​	用户级线程的切换在用户空间即可完成，不需要切换到内核态，效率更高；

- **缺点：**

​	当一个用户级线程被阻塞后，整个进程都会被阻塞，并发度不高。多个线程不可在多核处理机上并行运行；



#### 6.7.2 内核级线程

![image-20240328191052348](Images\image-20240328191052348.png)



#### 6.7.3 一对一模型

![image-20240328191234494](Images\image-20240328191234494.png)

#### 6.7.4 多对一模型

**退化**为了纯用户级线程

![image-20240328191338916](Images\image-20240328191338916.png)



#### 6.7.5 多对多模型

![image-20240328191846426](Images\image-20240328191846426.png)



## 7. 内存

### 7.1 内存管理

- 操作系统负责**内存空间的分配和回收**；
- 操作系统需要提供某种技术从逻辑上**对内存空间进行扩充（如虚拟地址）**；
- 操作系统需要提供地址转换功能，负责程序的**逻辑地址**和**物理地址**的转换；

![image-20240329033719898](Images\image-20240329033719898.png)

- 操作系统要负责**进程保护**，防止进程访问越界

![image-20240329034006584](Images\image-20240329034006584.png)





### 7.2 覆盖与交换

****

***覆盖技术***

****

![image-20240329211006035](Images\image-20240329211006035.png)

![image-20240329211640978](Images\image-20240329211640978.png)



****

***交换技术***

****

![image-20240329212312757](Images\image-20240329212312757.png)



### 7.3 内存空间的分配与回收

#### 7.3.1 连续分配方式

****

***单一连续分配***

****

![image-20240329212709448](Images\image-20240329212709448.png)

****

***固定分区分配***

****

![image-20240329213016831](Images\image-20240329213016831.png)

![image-20240329213141051](Images\image-20240329213141051.png)

****

***动态分区分配***

****

![image-20240329220200902](Images\image-20240329220200902.png)

![image-20240329220331926](Images\image-20240329220331926.png)

![image-20240329220442437](Images\image-20240329220442437.png)

动态分配内存回收机制和**之前写的堆处理器的回收机制很类似**，之前写的堆处理器的匹配机制是首次适应算法

**动态分区分配算法**常见的有**首次适应算法，最佳适应算法，最坏适应算法，邻近适应算法**

****

- **首次适应算法**（**效果最好**）

**算法思想：**每次都从低地址开始查找，找到第一个能满足大小的空闲分区。

**如何实现：** **空闲分区以地址递增的次序排列**。每次分配内存时顺序查找**空闲分区链**(或**空闲分区表**)，找到大小能满足要求的第一个空闲分区。

****

- **最佳适应算法**

**算法思想：**由于动态分区分配是一种连续分配方式，为各进程分配的空间必须是连续的一整片区域。因此为了保证当“大进程”到来时能有连续的大片空间，可以尽可能多地留下大片的空闲区即，优先使用更小的空闲区。

**如何实现：**空闲分区**按容量递增次序链接**。每次分配内存时顺序查找**空闲分区链**(或**空闲分区表**)，找到大小能满足要求的第一个空闲分区。

**缺点：**每次都选最小的分区进行众配，会留下越来越多的、很小的、难以利用的内存块。因此这种方法会**产生很多的外部碎片**。

****

- **最坏适应算法**

又称 **最大适应算法(Largest Fit)**

**算法思想：**为了解决最佳适应算法的问题--即留下太多难以利用的小碎片，可以在每次分配时优先使用最大的连续空闲区，这样分配后剩余的空闲区就不会太小，更方便使用。

**如何实现：**空闲分区**按容量递减次序链接**。每次分配内存时顺序查找**空闲分区链**(或**空闲分区表**)，找到大小能满足要求的第一个空闲分区。

**缺点：**每次都选最大的分区进行分配，虽然可以让分配后留下的空闲区更大，更可用，但是这种方式会**导致较大的连续空闲区被迅速用完**。如果之后有“大进程”到达，就没有内存分区可用了。

****

- **邻近适应算法**

**算法思想：**首次适应算法每次都从链头开始查找的。这可能会导致低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。如果每次都从上次查找结束的位置开始检索，就能解决上述问题。

**如何实现：**空闲分区以地址递增的顺序排列(**可排成一个循环链表**)。每次分配内存时**从上次查找结束的位置开始**查找**空闲分区链**(或**空闲分区表**)，找到大小能满足要求的第一个空闲分区。

**缺点：** 

**首次适应算法**每次都要从头查找，每次都需要检索低地址的小分区但是这种规则也决定了当低地址部分有更小的分区可以满足需求时会更有可能用到低地址部分的小分区，也会更有可能把高地址部分的大分区保留下来(**最佳适应算法的优点**)

**邻近适应算法**的规则可能会导致无论低地址、高地址部分的空闲分区都有相同的概率被使用，也就导致了**高地址部分的大分区更可能被使用**，划分为小分区，最后导致**无大分区可用**(**最坏适应算法的缺点**)



#### 7.3.2 基本分页存储管理

分页存储是**非连续存储方式**的一种

![image-20240330161416038](Images\image-20240330161416038.png)

**页，页面：**虚拟地址空间切出的内容

**页框，页帧，内存块：**物理地址空间切出的存放区域

![image-20240330161748429](Images\image-20240330161748429.png)

****

- ***每个页表项占多少字节？***

![image-20240330162305725](Images\image-20240330162305725.png)

****

- ***如何实现地址的转换***

![image-20240330162519999](Images\image-20240330162519999.png)

**如果确认一个逻辑地址对应的页号、页内偏移量**？

![image-20240330162750894](Images\image-20240330162750894.png)

****

- ***为何页面大小要取2的整数幂？***

![image-20240330163238171](Images\image-20240330163238171.png)

![image-20240330164139704](Images\image-20240330164139704.png)

**如何计算：**

**页号**=逻辑地址/页面长度(取除法的**整数**部分)

**页内偏移量**=逻辑地址%页面长度(取除法的**余数**部分)



**总结：**

- 逻辑地址的拆分更加迅速--如果每个页面大小为 $2^k$ B，用二进制数表示逻辑地址，则**末尾K位**即为**页内偏移量**，其余部分就是**页号**。因此，如果**让每个页面的大小为2的整数幂**，计算机硬件就可以很方便地得出一个逻辑地址对应的页号和页内偏移量，而无需进行除法运算，从而提升了运行速度。
- 物理地址的计算更加迅速--根据逻辑地址得到页号，根据页号查询页表从而找到页面存放的内存块号，将**二进制表示的内存块号和页内偏移量拼接起来**，就可以得到最终的物理地址。

****

- ***逻辑地址结构***

![image-20240330164819836](Images\image-20240330164819836.png)



##### 7.3.2.1 地址变换机构

##### 7.3.2.2 两级页表

****

- ***单级页表的问题***

**问题一：**页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框。

**问题二：**没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。

**思考：我们是如何解决进程在内存中必须连续存储的问题的?**

答：将进粽地址空间分页，并为其建立一张页表，记录各页面的存放位置



同样的思路也可用于解决“页表必须连续存放”的问题，把**必须连续存放的页表再分页**

****

- ***两级页表的原理、地址结构***

![image-20240330170212094](Images\image-20240330170212094.png)



#### 7.3.3 基本分段存储管理

**进程的地址空间：**按照程序**自身的逻辑**关系**划分为若干个段**，每个段都有一个段名(在低级语言中，程序员使用段名来编程)，**每段从0开始编址**

**内存分配规则：**以段为单位进行分配，**每个段在内存中占据连续空间**，但**各段之间可以不相邻**。

![image-20240330172315043](Images\image-20240330172315043.png)

![image-20240330182039132](Images\image-20240330182039132.png)

****

- ***分页和分段管理对比***

**页**是**信息的物理单位**。分页的主要目的是为了实现离散分配，提高内存利用率。分页仅仅是系统管理上的需要，完全是系统行为，**对用户是不可见的**。

**段**是**信息的逻辑单位**。分段的主要目的是更好地满足用户需求。一个段通常包含着一组属于一个逻辑模块的信息。**分段对用户是可见的**，用户编程时需要显式地给出段名。

页的大小固定且由系统决定。段的长度却不固定，决定于用户编写的程序。

**分页**的用户进程**地址空间是一维的**，程序员只需给出一个记忆符即可表示一个地址。

**分段**的用户进程**地址空间是二维的**，程序员在标识一个地址时，既要给出段名，也要给出段内地址。

![image-20240330184356965](Images\image-20240330184356965.png)

**分段**比分页**更容易实现信息的共享和保护**

![image-20240330193725313](Images\image-20240330193725313.png)



#### 7.3.4 段页式管理

|          | 优点                                                         | 缺点                                                         |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 分页管理 | 内存空间利用率高，**不会产生外部碎片**，只会有少量的页内碎片 | 不方便按照逻辑模块实现信息的共享和保护                       |
| 分段管理 | 很方便按照逻辑模块实现信息的共享和保护                       | 如果段长过大，为其分配很大的连续空间会很不方便。另外，段式管理**会产生外部碎片** |



**先分段，段内再分页 = 段页式管理**

![image-20240330194654958](Images\image-20240330194654958.png)



### 7.4 虚拟内存

虚拟内存是一种**对内存空间的扩充**

在虚拟内存出现之前，如果要运行一个程序需要把它的所有内容都存储进内存中，这就要求内存的可用容量一定要大于这个程序所需容量，这就限制了程序的使用。对于像GTA5这种100GB的游戏程序，显然无法将它全都存储到内存中，因此引入了虚拟内存技术，虚拟内存技术**每次只将程序中需要运行的内容放入内存中**

#### 7.4.1 局部性原理

**时间局部性：**如果执行了程序中的某条指令，那么不久后这条指令很有可能再次执行;如果某个数据被访问过，不久之后该数据很可能再次被访问。(因为程序中存在大量的循环)

**空间局部性：**一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也很有可能被访问。(因为很多数据在内存中都是连续存放的，并且程序的指令也是顺序地在内存中存放的)

#### 7.4.2 定义和特征

**定义：**

基于局部性原理，在程序装入时，可以将程序中**很快会用到的部分装入内存**，**暂时用不到的部分留在外存**就可以让程序开始执行。

在程序执行过程中，当所访问的**信息不在内存时**，由**操作系统负责将所需信息从外存调入内存**，然后继续执行程序。 

若内存空间不够，由**操作系统负责**将内存中**暂时用不到的信息换出到外存**。

在操作系统的管理下，在用户看来似乎有一个比实际内存大得多的内存，这就是**虚拟内存**

**特征：**

- **多次性：**无需在作业运行时一次性全部装入内存，而是允许被分成多次调入内存；
- **对换性：**在作业运行时无需一直常驻内存，而是允许在作业运行过程中，将作业换入、换出；
- **虚拟性：**从逻辑上扩充了内存的容量，使用户看到的内存容量，远大于实际的容量；



#### 7.4.3 实现

![image-20240330200703651](Images\image-20240330200703651.png)



#### 7.4.4 请求分页存储管理

**请求分页**存储管理与**基本分页**存储管理的主要区别:

在程序执行过程中，当**所访问的信息不在内存时，由操作系统负责将所需信息从外存调入内存**，然后继续执行程序。

若内存空间不够，由操作系统负责**将内存中暂时用不到的信息换出到外存**

##### 7.4.4.1 页表机制

![image-20240330201600909](Images\image-20240330201600909.png)

##### 7.4.4.2 缺页中断

在请求分页系统中，当要访问的**页面不在内存**中时便产生了一个**缺页中断**，交给操作系统的**缺页中断处理程序处理中断**。此时**缺页的进程阻塞**进入阻塞队列，调页完成后**将其唤醒**放回就绪队列



缺页中断存在两种情况：

- 如果内存中**有空闲块**，则为进程**分配一个空闲块**，将所缺页面装入该块，并修改页表中相应的页表项。
- 如果内存中**没有空闲块**，则由**页面置换算法**选择一个页面淘汰，若该页面在内存期间**被修改过**，则要将其**写回外存**。未修改过的页面不用写回外存。



**缺页中断**是因为当前执行的指令想要访问的目标页面未调入内存而产生的，因此**属于内中断**



##### 7.4.4.3 页面置换算法

- ### LRU（最近最久未使用，Least Recently Used）算法

LRU算法基于这样一种假设：如果数据最近被访问过，那么将来被再次访问的概率也很高。因此，LRU算法会优先淘汰那些最长时间未被访问的数据。

**实现方法：**

- 一个常见的LRU缓存实现方式是使用一个哈希表加上一个双向链表。哈希表用于保证查找速度，双向链表用于记录元素的访问顺序，链表头部是最近访问的，尾部是最久未访问的。
- 当一个新数据被访问时，如果缓存未满，将其添加到链表头部；如果缓存已满，则删除链表尾部的数据，并将新数据添加到头部。
- 如果缓存中的数据被再次访问，这个数据会被移动到链表头部。

**应用场景：** 适用于最近的数据访问模式较为频繁的情况。

~~~c++
struct DLinkedNode{
	int key, val;
    DLinkedNode* prev, * next;
    DLinkedNode() : key(0), val(0), prev(nullptr), next(nullptr){}
    DLinkedNode(int i_key, int i_val) : key(i_key), val(i_val), prev(nullptr), next(nullptr){}
};

class LRU{
public:
    LRU(int i_capacity){
        capacity = i_capacity;
        size = 0;
        head = new DLinkedNode();
        tail = new DLinkedNode();
       	head->next = tail;
        tail->prev = head;
    }
    
    int get(int i_key){
        if(umap.find(i_key) != umap.end()){
            MoveToHead(umap[i_key]);
            return umap[i_key]->val;
        }
        else return -1;
    }
    
    void put(int key, int value){
        if(umap.find(key) != umap.end()){
            umap[key]->val = value;
            MoveToHead(umap[key]);
        }
        else{
            DLinkedNode* newNode = new DLinkedNode(key, value);
            AddToHead(newNode);
            if(size == capacity){
                DLinkedNode* TailNode = DeleteTail();
                umap.erase(TailNode->key);
            }
            else{
                size++;
            }
        }
    }
    
    void AddToHead(DLinkedNode* node){
        node->next = head->next;
        node->prev = head;
        head->next->prev = node;
        head->next = node;
    }
    
    void MoveToHead(DLinkedNode* node){
        node->prev->next = node->next;
        node->next->prev = node->prev;
        node->next = head->next;
        head->next->prev = node;
        head->next = node;
        node->prev = head;
    }
    
    DLinkedNode* DeleteTail(){
        DLinkedNode* TailNode = tail->prev;
        TailNode->prev->next = tail;
        tail->prev = TailNode->prev;
        
        return TailNode;
    }
    
private:
    int capacity, size;
    DLinkedNode* head, * tail;
    unordered_map<int, DLinkedNode*> umap;
}
~~~



- ### FIFO（First In First Out）算法

FIFO算法是最简单的缓存算法，按照数据进入缓存的顺序来淘汰数据。最先进入缓存的数据，当缓存满时，也将是最先被淘汰的。

**实现方法：**

- 可以使用一个队列来实现FIFO缓存。新访问的数据被添加到队列的末尾。
- 当缓存达到最大容量时，队列头部的数据（即最早进入缓存的数据）会被移除。

**应用场景：** FIFO算法实现简单，但不考虑数据的访问模式，因此适用性较为有限，可能用在一些对缓存淘汰策略要求不高的场景。

- ### LFU（最近最少使用算法，Least Frequently Used）算法

LFU算法根据数据的访问频次来进行淘汰，优先淘汰访问频次最低的数据。与LRU不同，LFU关注的是访问的频率。

**实现方法：**

- LFU缓存的实现较为复杂，通常需要维护一个按访问频率组织的数据结构，比如最小堆，以及一个哈希表来存储频率。
- 每次访问数据时，数据的访问频率会更新，数据结构也随之调整以保持正确的顺序。
- 当缓存满时，频率最低的数据会被淘汰。

**应用场景：** 适用于需要根据长期访问模式来优化的场景，但由于其实现复杂度较高，不适用于访问模式快速变化的环境。

- ### ARC（自适应缓存替换，Adaptive Replacement Cache）算法

ARC算法结合了LRU和LFU的特点，自适应地调整对最近使用和频繁使用的数据的偏好。它维护两个LRU列表，一个记录最近访问的数据，另一个记录频繁访问的数据，并动态地调整这两个列表的大小。这种自适应特性使得ARC能够在不同的工作负载下都表现出良好的性能。

- ### 2Q算法

2Q算法是一种结合了FIFO和LRU优点的缓存替换算法。它将缓存分为两个队列，一个是FIFO队列，另一个是LRU队列。新访问的数据首先进入FIFO队列，如果数据在FIFO队列中被再次访问，则将其移动到LRU队列。当需要淘汰数据时，首先从FIFO队列中淘汰，这种方法既考虑了数据的新鲜度也考虑了数据的访问频率。

# 常见问题

## 1. 同步，异步，阻塞，非阻塞

[深入理解同步阻塞、同步非阻塞、异步阻塞、异步非阻塞_同步阻塞 同步非阻塞 异步阻塞 异步非阻塞-CSDN博客](https://blog.csdn.net/wangpaiblog/article/details/117236684#:~:text=同步阻塞：在需要某资源时马上发起请求，并暂停本线程之后的程序，直至获得所需的资源。 同步非阻塞：在需要某资源时马上发起请求，且可以马上得到答复，然后继续执行之后的程序。,但如果得到的不是完整的资源，之后将周期性地的请求，直至获得所需的资源。 异步阻塞：在需要某资源时不马上发起请求，而安排一个以后的时间再发起请求。)

**同步：**指现在在做某件事，但是突然有另一件事需要做，于是我立刻去做另一件事

**异步：**指现在在做某件事，但是突然有另一件事需要做，可是我不急，等会再去做另一件事，现在先继续做之前的事

**阻塞：**我去做另一件事了，在拿到结果前我什么也不做

**非阻塞：**我去做另一件事了，在等待结果时继续做别的事，时不时看看结果出没出来

简单总结就是同步异步**关心的是某件事要不要立刻做**，阻塞非阻塞关心的是**开始做某事了我要不要在那干等结果不干别的**



- **同步阻塞：**在需要某资源时马上发起请求，并暂停本线程之后的程序，直至获得所需的资源。
- **同步非阻塞：**在需要某资源时马上发起请求，且可以马上得到答复，然后继续执行之后的程序。但如果得到的不是完整的资源，之后将周期性地的请求，直至获得所需的资源。
- **异步阻塞：**在需要某资源时不马上发起请求，而安排一个以后的时间再发起请求。当到了那时发出请求时，将暂停本线程之后的程序，直至获得所需的资源。在获取资源之后，使用共享信号量、异步回调等方式将结果异步反馈。
- **异步非阻塞：**在需要某资源时不马上发起请求，而安排一个以后的时间再发起请求。当到了那时发出请求时，可以马上得到答复，然后继续执行之后的程序。但如果得到的不是完整的资源，之后将周期性地的请求。在最终获取到资源之后，使用共享信号量、异步回调等方式将结果异步反馈。



## 2. 互斥锁，自旋锁和读写锁的区别

- **互斥锁（Mutex）：**互斥锁使用到了互斥量（二元信号量），其保证在任意时刻只有一个线程能够进入被保护的临界区。当一个线程获取到互斥锁后，其他线程若要进入临界区会被阻塞，直到该线程释放锁。互斥锁是一种阻塞锁，当线程无法获取到锁时，会进入阻塞状态。使用互斥锁需要**成对使用PV操作**，如果**只使用P操作会导致资源无法释放**，如果**只使用V操作会导致无法实现互斥访问临界资源**
- **自旋锁（Spinlock）：**自旋锁是一种忙等待锁，当一个线程发现自旋锁被其他线程占用时，它会一直循环等待而不进入阻塞状态，直到该自旋锁可用。自旋锁是一种非阻塞锁，线程在等待锁期间会一直占用 CPU 资源进行循环检测。**自旋锁可以是抢占式或非抢占式的**，要注意非抢占式自旋锁根据使用算法的不同可能无法用在单处理机系统中
- **读写锁（ReadWrite Lock）：**读写锁允许多个线程同时对共享数据进行读操作，但只允许单个线程进行写操作。当有线程正在写入时，其他线程无法进行读操作，防止数据不一致性。读写锁允许多个线程并发读，但只能允许单个线程进行写操作。写操作时需要独占锁，阻塞其他线程的读写操作。

应用场景上，
互斥锁适用于临界区资源访问时间较长或存在阻塞操作的情况
自旋锁适用于临界区资源访问时间短，且线程竞争不激烈的情况
读写锁适用于读操作远远多于写操作的场景，可以提高并发读性能